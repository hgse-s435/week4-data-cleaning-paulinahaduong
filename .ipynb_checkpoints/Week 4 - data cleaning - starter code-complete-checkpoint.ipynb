{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the graphs inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import path\n",
    "\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week we are going to work with some text data. In this folder, you should a text file called 'fullpapers.txt'. This file was generated by converting the proceedings of the EDM (Educational Data Mining) conference of 2018. You can find the proceedings here: http://educationaldatamining.org/EDM2017/proc_files/fullpapers.pdf\n",
    "We are going to explore the different terms that are used by authors of the papers in this conference, which will require some data cleaning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is compare the different papers in terms of the vocabulary used. \n",
    "* open the pdf of the proceedings (fullpapers.pdf); \n",
    "* open the txt of the proceedigs (fullpapers.txt)\n",
    "\n",
    "1) we want to split the data into different papers. Brainstorm a few ideas on how to do that:\n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) First we are going to read the fullpapers.txt file \n",
    "# and assign its content to a variable called \"data\"\n",
    "# hint: https://stackoverflow.com/questions/3758147/easiest-way-to-read-write-a-files-content-in-python\n",
    "\n",
    "with open('fullpapers.txt') as f: data = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "# 3) To facilitate data processing, we want to split this file\n",
    "# into different pages. Create a list called \"pages\" that \n",
    "# stores the text presented on each page of the pdf\n",
    "# Look into the .split() function, what string are we going to want to split by?\n",
    "\n",
    "pages = []\n",
    "# pages = data.split(/\\s\\d{1,3}\\s{2}/)\n",
    "\n",
    "# pages = re.split('\\s\\d{1,3}\\s{2}',data)\n",
    "\n",
    "#WHY DON'T THESE WORK?!?!?!?!\n",
    "\n",
    "# pages = re.split('[\\r\\n]\\d{1,3}[\\r\\n]{2}',data)\n",
    "\n",
    "# print(pages)\n",
    "\n",
    "pages = data.split('Proceedings of the 10th International Conference on Educational Data Mining')\n",
    "\n",
    "print(len(pages))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) because we don't want to deal with upper case / lower case issues\n",
    "# we are going to lower case everything:\n",
    "# Try using a list comprehension to accomplish this task\n",
    "\n",
    "# pagesLower = []\n",
    "\n",
    "# for page in pages:\n",
    "#     page = page.lower()\n",
    "#     pagesLower.append(page)\n",
    "    \n",
    "pages = [x.lower() for x in pages]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Now we would like to join pages if they below to the same paper. Can you think of keywords we could like for to decided if the current page is starting a new paper? Write down two ideas:\n",
    "1. abstract?\n",
    "2. references?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "# 6) create a new list called \"papers\", which is going to contain \n",
    "# all the papers we have. Iterate through all the pages and \n",
    "# add a new element to the list when you have a full paper\n",
    "# Using a for loop to iterate over all the pages, try to think of a conditional statement to check whether a page\n",
    "# represents a new 'paper'. I.e. what is a common aspect of all papers? \n",
    "papers = []\n",
    "# current_paper = pages[0]\n",
    "current_paper = ''\n",
    "\n",
    "for page in pages:\n",
    "    if 'abstract' in page and 'introduction' in page:\n",
    "        if current_paper != '':\n",
    "            papers.append(current_paper)\n",
    "            current_paper = page\n",
    "        else:\n",
    "           current_paper += page \n",
    "    else:\n",
    "        current_paper += page\n",
    "        \n",
    "print (len(papers))\n",
    "\n",
    "# print(papers[0])\n",
    "\n",
    "# papers.pop(0)\n",
    "\n",
    "# print(len(papers))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# iterate through the pages and add each paper to the list \"papers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "# 7) print how many files you have in the \"papers\" list:\n",
    "print(len(papers))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f",
      "zone out no more: mitigating mind wandering during\n",
      "computerized reading\n",
      "sidney k. dâ€™mello, caitlin mills, robert bixler, & nigel bosch\n",
      "university of notre dame\n",
      "118 haggar hall\n",
      "notre dame, in 46556, usa\n",
      "sdmello@nd.edu\n",
      "\n",
      "abstract\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "process\n",
      "\n",
      "\n",
      "15\n",
      "\n",
      "\f",
      "measuring similarity of educational items using data on\n",
      "learnersâ€™ performance\n",
      "jirÌŒÃ­ rÌŒihÃ¡k\n",
      "\n",
      "faculty of informatics\n",
      "masaryk university\n",
      "brno, czech republic\n",
      "\n",
      "thran@mail.muni.cz\n",
      "abstract\n",
      "educational systems typically contain a large pool of items\n",
      "(questions, problems). using data mining techniqu\n"
     ]
    }
   ],
   "source": [
    "# 8) print the content of the first two paper to make sure it worked\n",
    "# (only print the first 300 characters)\n",
    "\n",
    "# for paper in papers[0-2]:\n",
    "#     print (paper[:300])\n",
    "    \n",
    "# for paper in range(2): print(papers[paper][:300])\n",
    "\n",
    "print (papers[0][0:300])\n",
    "print (papers[1][0:300])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) create a new folder called papers; this is where we are \n",
    "# going to save each paper into a separate text file\n",
    "# hint: google \"how to create a new folder with python\"\n",
    "\n",
    "newpath = 'papers' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) save each paper into its unique file in the \"Papers\" folder\n",
    "# we created above\n",
    "# Hint: \"enumerate\" can provide you with the index of the paper in the list\n",
    "# Feel free to use the following filename for the first paper in the list:\n",
    "# ./Papers/paper0.txt on mac and .\\Papers\\paper0.txt on windows\n",
    "\n",
    "for i, paper in enumerate(papers,0):\n",
    "    file = open('./Papers/paper' + str(i) + '.txt','w') \n",
    "    file.write(paper) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be asking yourself why we need to save the data into text files (instead of just using the list of papers above). One answer is that when we work with large datastsets, it's useful to save snapshots of our data that is \"clean\". This way we don't have to re-run all the code above and we save time. It also allows us to share data between different notebooks for other types of analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Papers/paper12.txt',\n",
       " 'Papers/paper5.txt',\n",
       " 'Papers/paper4.txt',\n",
       " 'Papers/paper13.txt',\n",
       " 'Papers/paper11.txt',\n",
       " 'Papers/paper6.txt',\n",
       " 'Papers/paper7.txt',\n",
       " 'Papers/paper10.txt',\n",
       " 'Papers/paper14.txt',\n",
       " 'Papers/paper3.txt',\n",
       " 'Papers/paper2.txt',\n",
       " 'Papers/paper15.txt',\n",
       " 'Papers/paper0.txt',\n",
       " 'Papers/paper1.txt',\n",
       " 'Papers/paper16.txt',\n",
       " 'Papers/paper9.txt',\n",
       " 'Papers/paper8.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11) We are going to practice your \"glob\" skills - find all the \n",
    "# text files in the \"Papers\" folder with a glob command!\n",
    "\n",
    "import glob\n",
    "\n",
    "glob.glob('Papers/*.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "103\n",
      "\n",
      "\f",
      "epistemic network analysis and topic modeling for chat\n",
      "data from collaborative learning environment\n",
      "zhiqiang cai\n",
      "\n",
      "brendan eagan\n",
      "\n",
      "nia m. dowell\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 410\n",
      "memphis, tn, usa\n",
      "\n",
      "university of wisconsin-madison\n",
      "1025 west johnson street\n",
      "madison, wi, usa\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 410\n",
      "memphis, tn, usa\n",
      "\n",
      "zcai@memphis.edu\n",
      "\n",
      "eaganb@gmail.com\n",
      "\n",
      "niadowell@gmail.com\n",
      "\n",
      "james w. pennebaker\n",
      "\n",
      "david w. shaffer\n",
      "\n",
      "arthur c. graesser\n",
      "\n",
      "university of texas-austin\n",
      "116 inner campus dr stop g6000\n",
      "austin, tx, usa\n",
      "\n",
      "university of wisconsin-madison\n",
      "1025 west johnson street\n",
      "madison, wi, usa\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 403\n",
      "memphis, tn, usa\n",
      "\n",
      "pennebaker@utexas.edu\n",
      "\n",
      "dws@education.wisc.edu\n",
      "\n",
      "art.graesser@gmail.com\n",
      "\n",
      "abstract\n",
      "this study investigates a possible way to analyze chat data from\n",
      "collaborative learning environments using epistemic network\n",
      "analysis and topic modeling. a 300-topic general topic model\n",
      "built from tasa (touchstone applied science associates) corpus was used in this study. 300 topic scores for each of the 15,670\n",
      "utterances in our chat data were computed. seven relevant topics\n",
      "were selected based on the total document scores. while the aggregated topic scores had some power in predicting studentsâ€™\n",
      "learning, using epistemic network analysis enables assessing the\n",
      "data from a different angle. the results showed that the topic\n",
      "score based epistemic networks between low gain students and\n",
      "high gain students were significantly different (ğ‘¡ = 2.00). overall,\n",
      "the results suggest these two analytical approaches provide complementary information and afford new insights into the processes\n",
      "related to successful collaborative interactions.\n",
      "\n",
      "keywords\n",
      "chat; collaborative learning; topic modeling; epistemic network\n",
      "analysis\n",
      "\n",
      "1. introduction\n",
      "collaborative learning is a special form of learning and interaction\n",
      "that affords opportunities for groups of students to combine cognitive resources and synchronously or asynchronously participate in\n",
      "tasks to accomplish shared learning goals [15; 20]. collaborative\n",
      "learning groups can range from a pair of learners (called a dyad),\n",
      "to small groups (3-5 learners), to classroom learning (25-35 learners), and more recently large-scale online learning environments\n",
      "with hundreds or even thousands of students [5; 22]. the collaborative process provides learners with a more efficient learning\n",
      "experience and improves learnersâ€™ collaborative learning skills,\n",
      "which are critical competencies for students [14]. members in a\n",
      "team are different in many ways. they have their own experience,\n",
      "knowledge, skills, and approaches to learning. a student in a col-\n",
      "\n",
      "laborative learning environment can take other studentsâ€™ views\n",
      "and ideas about the information provided in the learning material.\n",
      "the ideas coming out of the team can then be integrated as a\n",
      "deeper understanding of the material, or a better solution to a\n",
      "problem.\n",
      "traditional collaborative learning occurred in the form of face to\n",
      "face group discussion or problem solving. as the internet and\n",
      "learning technologies develop, online collaborative learning environments come out and are playing more and more important\n",
      "roles. for example, moocs (massive open online courses) have\n",
      "drawn massive number of learners. learners in moocs are connected by the internet and can easily interact with each other using\n",
      "various types of tools, such as forums, blogs and social networks\n",
      "[23]. these digitized environments make it possible to track the\n",
      "learning processes in collaborative learning environments in\n",
      "greater detail.\n",
      "communication is one of the main factors that differentiates collaborative learning from individual learning [4; 6; 9]. as such,\n",
      "chats from collaborative learning environments provide rich data\n",
      "that contains information about the dynamics in a learning process. understanding massive chat data from collaborative learning\n",
      "environments is interesting and challenging. many tools have\n",
      "been invented and used in chat data analysis, such as liwc (linguistic inquiry and word count) [12], coh-metrix [10], and topic\n",
      "modeling, just to name a few. epistemic network analysis (ena)\n",
      "has been playing a unique role in analyzing chat data from epistemic games [18]. ena is rooted in a specific theory of learning:\n",
      "the epistemic frame theory, in which the collection of skill,\n",
      "knowledge, identity, value and epistemology (skive) forms an\n",
      "epistemic frame. a critical theoretical assumption of ena is that\n",
      "the connections between the elements of epistemic frames are\n",
      "critical for learning, not their presence in isolation. the online\n",
      "ena toolkit allows users to analyze chat data by comparing the\n",
      "connections within the epistemic networks derived from chats.\n",
      "ena visualization displays the clustering of learners and groups\n",
      "and the network connections of individual learners and groups.\n",
      "ena requires coded data which has traditionally relied on hand\n",
      "coded data sets or classifiers that rely on regular expression mapping. combining topic modeling with ena will provide a new\n",
      "mode of preparing data sets for analysis using ena.\n",
      "in this study, we used a combination of topic modeling and ena\n",
      "to analyze chat data to see if we could detect differences between\n",
      "the connections made by students with high learning gains versus\n",
      "students with low learning gains. incorporating topic modeling\n",
      "\n",
      "\n",
      "\n",
      "104\n",
      "\n",
      "\f",
      "with ena will make the analytic tool more fully automated and of\n",
      "greater use to the research community.\n",
      "\n",
      "2. related work\n",
      "chats have two obvious features. first, they appear in the form of\n",
      "text. therefore, any text analysis tool may have a role in chat\n",
      "analysis. second, chats come from individualsâ€™ interaction, which\n",
      "reflects social dynamics between participants. therefore, a combination of text analysis and social network analysis should be\n",
      "helpful in understanding underlying chat dynamics. for instance,\n",
      "tuulos et al. [21] combined topic modeling with social network\n",
      "analysis in chat data analysis. they found that topic modeling can\n",
      "help identify the receiver of chats (the person who a chat is given\n",
      "to).\n",
      "in a similar effort, scholand et al. [16] combined liwc and social\n",
      "network analysis to form a method called â€œsocial language network analysisâ€ (slna). the social networks were formed by\n",
      "counting the number of times chat occurred between any two\n",
      "participants. based on the counts, participants were clustered into\n",
      "a tree structure, representing the level of subgroups the participants belong to. liwc was then used to get the text features of\n",
      "chats. it was found that, some liwc features were significantly\n",
      "different between in group conversations and out of group conversations.\n",
      "researchers have also recently explored the advantages of combining sna (social network analysis) with deeper level computational linguistic tools, like coh-metrix. coh-metrix computes\n",
      "over 100 text features. the five most important coh-metrix features are: narrativity, syntax simplicity, word concreteness, referential cohesion and deep cohesion. dowell and colleagues [8]\n",
      "explored the extent to which characteristics of discourse diagnostically reveals learnersâ€™ performance and social position in\n",
      "moocs. they found that learners who performed significantly\n",
      "better engaged in more expository style discourse, with surface\n",
      "and deep level cohesive integration, abstract language, and simple\n",
      "syntactic structures. however, linguistic profiles of the centrally\n",
      "positioned learners differed from the high performers. learners\n",
      "with a more significant and central position in their social network\n",
      "engaged using a more narrative style discourse with less overlap\n",
      "between words and ideas, simpler syntactic structures and abstract\n",
      "words. an increasing methodological contribution of this work\n",
      "highlights how automated linguistic analysis of student interactions can complement social network analysis (sna) techniques\n",
      "by adding rich contextual information to the structural patterns of\n",
      "learner interactions.\n",
      "\n",
      "final sample. within the population, 50.5% of the sample identified as caucasian, 22.2% as hispanic/latino, 15.4% as asian\n",
      "american, 4.4% as african american, and less than 1% identified\n",
      "as either native american or pacific islander.\n",
      "course details and procedure. students were told that they\n",
      "would be participating in an assignment that involved a collaborative discussion on personality disorders and taking quizzes. students were told that their assignment was to log into an online\n",
      "educational platform specific to the university at a specified time,\n",
      "where they would take quizzes and interact via web chat with one\n",
      "to four random group members. students were also instructed\n",
      "that, prior to logging onto the educational platform, they would\n",
      "have to read material on personality disorders. after logging into\n",
      "the system, students took a 10 item, multiple choice pretest quiz.\n",
      "this quiz asked students to apply their knowledge of personality\n",
      "disorders to various scenarios and to draw conclusions based on\n",
      "the nature of the disorders. the following is an example of the\n",
      "types of quiz questions students were exposed to:\n",
      "ï‚·\n",
      "ï‚·\n",
      "\n",
      "ï‚·\n",
      "\n",
      "jacob was diagnosed with narcissistic personality disorder. why might dr. simon think this was the wrong\n",
      "diagnosis?\n",
      "dr. level has measured and described his 10 mice of\n",
      "varying ages in terms of their length (cm) and weight\n",
      "(g). how might he describe them on these characteristics using a dimensional approach?\n",
      "danielle checks her facebook page every hour. does\n",
      "danielle have narcissistic personality disorder?\n",
      "\n",
      "after completing the quiz, they were randomly assigned to other\n",
      "students who were waiting to engage in the chatroom portion of\n",
      "the task. when there were at least 2 students and no more than 5\n",
      "students (m = 4.59), individuals were directed to an instant messaging platform that was built into the educational platform. the\n",
      "group chat began as soon as someone typed the first message and\n",
      "lasted for 20 minutes. the chat window closed automatically after\n",
      "20 minutes, at which time students took a second 10 multiplechoice question quiz. each student contributed 154.0 words on\n",
      "average (sd = 104.9) in 19.5 sentences (sd = 12.5). as a group,\n",
      "discussions were about 714.8 words long (sd = 235.7) and 90.6\n",
      "sentences long (sd = 33.5).\n",
      "an excerpt of a collaborative interaction chat in a chat room is\n",
      "shown below in table 1. (student names have been changed):\n",
      "table 1. an excerpt of a collaborative interaction chat\n",
      "student\n",
      "\n",
      "chat text\n",
      "\n",
      "in another study, dowell et al. [7] showed that studentsâ€™ linguistic\n",
      "characteristics, namely higher degrees of narrativity and deep\n",
      "cohesion, are predictive of their learning. that is, students engaged in deep cohesive interactions performed better.\n",
      "\n",
      "art\n",
      "\n",
      "ok cool, everyone's here. sooo first question\n",
      "\n",
      "art\n",
      "\n",
      "ok so the certain characteristics to be considered to\n",
      "have a personality disorder?\n",
      "\n",
      "in the present research, we explore collaborative interaction chat\n",
      "data using the combination of topic modeling and epistemic network analysis. while previous studies focused on the relationship\n",
      "between language features and social network connections, our\n",
      "study focuses on prediction learning performance by semantic\n",
      "network connections students make in chats.\n",
      "\n",
      "shaffer\n",
      "\n",
      "alright sooo first question: based on these criteria describe several reasons why a psychologist might not\n",
      "label someone with grandiose thoughts as having narcissistic personality disorder?\n",
      "\n",
      "shaffer\n",
      "\n",
      "hahaha never mind\n",
      "\n",
      "shaffer\n",
      "\n",
      "that was the second question.\n",
      "\n",
      "3. methods\n",
      "\n",
      "art\n",
      "\n",
      "lol its all good\n",
      "\n",
      "shaffer\n",
      "\n",
      "okay so certain characteristics: doesn't it have to be like\n",
      "a stable thing?\n",
      "\n",
      "carl\n",
      "\n",
      "i think the main thing about having a disorder is that its\n",
      "disruptive socially and/or makes the person a danger to\n",
      "himself or others\n",
      "\n",
      "participants. participants were enrolled in an introductory-level\n",
      "psychology course taught in the fall semester of 2011 at a large\n",
      "university in the usa. while 854 students participated in this\n",
      "course, some minor data loss occurred after removing outliers and\n",
      "those who failed to complete the outcome measures. the final\n",
      "sample consisted of 844 students. females made up 64.3% of this\n",
      "\n",
      "\n",
      "\n",
      "105\n",
      "\n",
      "\f",
      "vasile\n",
      "\n",
      "yes, stable over time\n",
      "\n",
      "shaffer\n",
      "\n",
      "yeah, and it also mentioned it can't be because of drugs\n",
      "\n",
      "art\n",
      "\n",
      "also they have to have like unrealistic fantasies\n",
      "\n",
      "nia\n",
      "\n",
      "yeah and not normal in their culture\n",
      "\n",
      "carl\n",
      "\n",
      "no drugs or physical injury\n",
      "\n",
      "vasile\n",
      "\n",
      "begins in early adulthood or adolescence\n",
      "\n",
      "shaffer\n",
      "\n",
      "i think that covers them? haha\n",
      "\n",
      "art\n",
      "\n",
      "ok, so arrogance doesn't just define it, they have to have\n",
      "most of these characteristics\n",
      "\n",
      "art\n",
      "\n",
      "yeah i think we got them\n",
      "\n",
      "shaffer\n",
      "\n",
      "is it most or is it like 6?\n",
      "\n",
      "from the above excerpt, we can see several obvious things. first,\n",
      "the lengths of the utterances varied from one single word to multiple sentences. this needs to be considered in text analysis because some methods work only for longer texts. for example,\n",
      "coh-metrix usually works well for texts with more than 200\n",
      "words. topic modeling also needs enough length to reliably infer\n",
      "topic scores. second, the number of utterances each participant\n",
      "gave were different. from how much and what a member said, we\n",
      "can see each member played a different role in that chat. third,\n",
      "the ordered sequence of the utterances forms a time series. understanding and visualizing the underlying discourse dynamics are\n",
      "important for meaning making with this type of data.\n",
      "the data set contained 15,670 utterances, pretest scores (the first\n",
      "quiz) and post test scores (the second quiz) for 844 students,\n",
      "grouped in 182 chat rooms. each chat room had 2 to 5 students,\n",
      "4.73 by average. the average speech turns each student gave was\n",
      "18.2 and the average speech turns in each room was 86.1.\n",
      "the average pretest score was 36.01% correct and the average\n",
      "post-test scores 45.73% correct. paired sample test shows that the\n",
      "post-test is significantly higher (ğ‘¡ = 14.13, ğ‘ = 844). we computed the learning gain of each student, using the formula\n",
      "ğ‘”ğ‘ğ‘–ğ‘› =\n",
      "\n",
      "ğ‘ğ‘œğ‘ ğ‘¡ğ‘¡ğ‘’ğ‘ ğ‘¡ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ âˆ’ ğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘’ğ‘ ğ‘¡ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’\n",
      "1âˆ’ğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘’ğ‘ ğ‘¡ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’\n",
      "\n",
      ".\n",
      "\n",
      "for all students (ğ‘ = 844), the average learning gain is 0.11,\n",
      "59.5% had positive learning gains above 0.1. 16.5% had the same\n",
      "scores and 23% had negative learning gains. not surprisingly,\n",
      "students who had lower pretest scores had higher learning gains\n",
      "because they had greater potential to learn. figure 1 shows the\n",
      "average learning gain as function of pretest score.\n",
      "\n",
      "0.6\n",
      "0.4\n",
      "\n",
      "this data set has been analyzed in multiple studies. cade et al. [3]\n",
      "analyzed the cohesion of the chats and found that deep cohesion\n",
      "of the chats predicts the students feeling of power and connectedness to the group. dowell et al. [7] found that some coh-metrix\n",
      "measures predicts learning. coh-metrix measures describe common textual features that are not content specific. for example,\n",
      "cohesion is about how text segments are semantically linked to\n",
      "each other, which has nothing to do with what the text content is\n",
      "about. in this study, we use topic modeling to provide content\n",
      "dependent features and use epistemic network analysis to explore\n",
      "how the topics were associated in the chats.\n",
      "\n",
      "4. topic modeling\n",
      "topic modeling has been widely used in text analysis to find what\n",
      "topics are in a text and what proportion/amount of each topic is\n",
      "contained. latent dirichlet allocation (lda) [2; 24] is one of the\n",
      "most popular methods for topic modeling. lda uses a generative\n",
      "process to find topic representations. lda starts from a large\n",
      "document set ğ· = {ğ‘‘1 , ğ‘‘2 , â‹¯ , ğ‘‘ğ‘š }. a word list ğ‘Š =\n",
      "{ğ‘¤1 , ğ‘¤2 , â‹¯ , ğ‘¤ğ‘› } is then extracted from the document set. lda\n",
      "assumes that the document set contains a certain number of topics,\n",
      "say, k topics. each document has a probability distribution over\n",
      "the k topics and each topic has a probability distribution over the\n",
      "given list of words. when a document was composed, each word\n",
      "that occurred in a document was assumed to be drawn based on\n",
      "the document-topic probability and the topic-word probability.\n",
      "for a given corpus (document set) and a given number of topics\n",
      "k, lda can compute the topic assignment of each word in each\n",
      "document.\n",
      "for a given topic, the word probability distribution can be easily\n",
      "computed from the number of times each word was assigned to\n",
      "the given topic. the beauty of topic modeling is that the â€œtop\n",
      "wordsâ€ (words with highest probabilities in a topic) usually give a\n",
      "meaningful interpretation of a topic. the distributions are the\n",
      "underlying representation of the topics. the top words are usually\n",
      "used to show what topics are contained in the corpus.\n",
      "by counting the number of words assigned to each topic, a topic\n",
      "proportion score can be computed for each document on each\n",
      "topic. the topic proportion scores then become a document feature that can be used in further analysis. however, the proportion\n",
      "scores are based on the statistical topic assignment of words.\n",
      "when documents are very short, such as most utterances in our\n",
      "chat data, the topic proportion scores wonâ€™t be reliable. cai et al.\n",
      "[4] argued that alternative ways to compute document topic scores\n",
      "are possible.\n",
      "\n",
      "4.1 tasa topic model\n",
      "\n",
      "0.2\n",
      "0.0\n",
      "-0.2\n",
      "\n",
      "for students with pretest scores less than 50% correct (n=624),\n",
      "the average learning gain is 0.88, 69.7% had positive learning\n",
      "gains, 15.7% had the same scores and 14.6% had negative learning gains.\n",
      "\n",
      ".00 .10 .20 .30 .40 .50 .60 .70 .80\n",
      "\n",
      "-0.4\n",
      "-0.6\n",
      "figure 1. average learning gain as a function of pretest score.\n",
      "\n",
      "although our chat data set contained 15,670 utterances, the utterances were short and the corpus is not large enough to build a\n",
      "reliable topic model. to get a reliable model, we used a well\n",
      "known corpus provided by tasa (touchstone applied science\n",
      "associates). this corpus contained documents on seven known\n",
      "categories, including business, health, home economics, industrial\n",
      "arts, language arts, science and social studies. our content topic,\n",
      "personality disorders, is obviously in the health category. of\n",
      "course, not all topics in tasa are relevant to our study. therefore, after building up the model, we need to select relevant topics. we will cover that in the next sub-section.\n",
      "\n",
      "\n",
      "\n",
      "106\n",
      "\n",
      "\f",
      "there are a total of 37,651 documents in tasa corpus, each of\n",
      "which is about 250 words long. before we ran lda, we filtered\n",
      "out very high frequency words and very low frequency words.\n",
      "high frequency words, such as â€œtheâ€, â€œofâ€, â€œinâ€, etc., wonâ€™t contain much topic information. rare words wonâ€™t contribute to\n",
      "meaningful statistics. 28,483 words (it might be better to say\n",
      "â€œtermsâ€) were left after filtering. a model with 300 topics was\n",
      "constructed by lda.\n",
      "\n",
      "4.2 topic score computation and topic selection\n",
      "from the tasa topic model, we computed the word-topic probabilities based on the number of times a word was assigned to each\n",
      "of the 300 topics. thus, each word is represented by a 300 dimensional probability distribution vector. for each chat in our chat\n",
      "corpus, we simply summed up the word probability vectors for the\n",
      "words appeared in each chat. that gave us 300 topic scores for\n",
      "each chat. recall that, the chats were associated with a reading\n",
      "material and two quizzes. while the students were free to talk\n",
      "about anything, the content of the reading material and the quizzes\n",
      "set up the main chat topics, that is, personality disorders.\n",
      "\n",
      "topic score\n",
      "1400\n",
      "1200\n",
      "1000\n",
      "800\n",
      "600\n",
      "\n",
      "200\n",
      "0\n",
      "0\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "figure 2. sorted topic scores for topic selection.\n",
      "the first thing we needed to do then was to investigate whether or\n",
      "not the â€œhotâ€ topics from the computation made sense. to find\n",
      "that out, we computed the sum of all topic scores over all chats.\n",
      "the topics were sorted according the total topic score. the hottest\n",
      "topic had a total score higher than 1300, much higher than the\n",
      "second highest (less than 900). by examining the top words, this\n",
      "topic is about â€œillnessâ€, which is highly relevant to personality\n",
      "disorders. six hot topics scored in the range from 600 to 900.\n",
      "they are about â€œoutdoorsâ€, â€œbiologyâ€, â€œpeople/socialâ€, â€œeducationâ€ and â€œhealthcareâ€. the top words are listed below.\n",
      "\n",
      "ï‚·\n",
      "ï‚·\n",
      "ï‚·\n",
      "\n",
      "ï‚·\n",
      "\n",
      "ï‚·\n",
      "\n",
      "â€œillnessâ€, â€œbiologyâ€, â€œpsychologyâ€ and â€œhealthcareâ€ are the topics\n",
      "the learning materials involved. â€œeducationâ€ topic is about the\n",
      "education environment where the chat happened. â€œoutdoorâ€ and\n",
      "â€œpeople/socialâ€ are off-task topics.\n",
      "to get an idea about whether or not the topic scores were related\n",
      "to the learning gain, we aggregated the scores by person and computed the correlation between the total topic score and the learning\n",
      "gain for each topic. we were only interested in looking at the\n",
      "students with larger potential to learn, so we removed the data\n",
      "with pretest score greater than or equal to 0.5, leaving 624 students out of 844. the results (table 1) showed that all topics were\n",
      "significantly correlated to learning gain. it doesnâ€™t seem to be\n",
      "great, because that seems to suggest that, whatever topic a student\n",
      "talked about, more a student talked, larger gain the student obtained. the real reason is that in the aggregation, all topic scores\n",
      "were summed up. therefore, all topic scores were influenced by\n",
      "the chat length. so the correlation in table 2 basically showed the\n",
      "chat length effect.\n",
      "table 2. correlation between total topic scores and learning\n",
      "gain (n=624, pretest<0.5)\n",
      "\n",
      "400\n",
      "\n",
      "ï‚·\n",
      "\n",
      "ï‚·\n",
      "\n",
      "person, animal, mental, response, positive, stress, personality, subject, reaction\n",
      "people/social: joe, pete, mr, charlie, dad, frank, billy,\n",
      "tony, jerry, 'll, mom, 'd, going, 're, got, boys, looked,\n",
      "asked, paper, go\n",
      "education: students, teacher, teachers, child, children,\n",
      "student, school, education, schools, learning, parents,\n",
      "tests, test, program, teaching, behavior, skills, reading,\n",
      "team, information\n",
      "healthcare: patient, doctor, health, hospital, medical,\n",
      "dr, patients, nurse, disease, doctors, team, care, office,\n",
      "nursing, drugs, medicine, services, dental, diseases, help\n",
      "\n",
      "illness: health, disease, patient, body, diseases, medical,\n",
      "stress, mental, physical, heart, doctor, problems, cause,\n",
      "person, patients, exercise, illness, problem, nurse,\n",
      "healthy\n",
      "outdoors: dog, energy, plants, earth, car, light, food,\n",
      "heat, words, animals, music, rock, language, children,\n",
      "air, uncle, city, sun, women, plant\n",
      "biology: cells, cell, genes, chromosomes, traits, color,\n",
      "organisms, sex, egg, species, gene, body, male, female,\n",
      "parents, nucleus, eggs, sperm, organism, sexual\n",
      "psychology: behavior, learning, theory, environment,\n",
      "feelings, sexual, physical, social, sex, human, research,\n",
      "\n",
      "topic\n",
      "\n",
      "post-test\n",
      "\n",
      "pretest\n",
      "\n",
      "gain\n",
      "\n",
      "illness\n",
      "\n",
      ".183**\n",
      "\n",
      ".116**\n",
      "\n",
      ".132**\n",
      "\n",
      "outdoors\n",
      "\n",
      ".216**\n",
      "\n",
      ".133**\n",
      "\n",
      ".154**\n",
      "\n",
      "biology\n",
      "\n",
      ".159**\n",
      "\n",
      ".125**\n",
      "\n",
      ".105**\n",
      "\n",
      "psychology\n",
      "\n",
      ".182**\n",
      "\n",
      ".096*\n",
      "\n",
      ".140**\n",
      "\n",
      "people/social\n",
      "\n",
      ".115**\n",
      "\n",
      ".022\n",
      "\n",
      ".107**\n",
      "\n",
      "education\n",
      "\n",
      ".175**\n",
      "\n",
      ".118**\n",
      "\n",
      ".121**\n",
      "\n",
      "healthcare\n",
      "\n",
      ".157**\n",
      "\n",
      ".130**\n",
      "\n",
      ".097*\n",
      "\n",
      "to remove the chat length effect, the simplest way is to divide all\n",
      "scores by the number of words (terms) in each chat. however, in\n",
      "this study, to be consistent with subsequent analysis, we normalized the topic scores to topic proportion scores by dividing each\n",
      "topic score for each utterance by the sum of all seven topic scores\n",
      "of the same utterance.\n",
      "the results (table 3) showed that the topic â€œpeople/socialâ€ had a\n",
      "significant negative correlation to learning gain. others were not\n",
      "significant but were in the direction we would expect. â€œillnessâ€,\n",
      "â€œbiologyâ€, â€œpsychologyâ€ and â€œhealthcareâ€ were positively correlated with gain scores, while â€œoutdoorsâ€ and â€œpeople/socialâ€ topics were negatively correlated with gains scores. we observed\n",
      "almost no correlation for the â€œeducationâ€ topic. this seems to\n",
      "indicate that the aggregated topic scores have limited power in\n",
      "predicting learning. therefore, we used ena to examine the connections or association of these topics in the students discourse to\n",
      "\n",
      "\n",
      "\n",
      "107\n",
      "\n",
      "\f",
      "develop a predictive model of learning gains based on the use of\n",
      "these topics.\n",
      "table 3. correlation between normalized topic proportion\n",
      "scores and learning gain (n=624, pretest<0.5)\n",
      "topic\n",
      "\n",
      "post-test\n",
      "\n",
      "pretest\n",
      "\n",
      "gain\n",
      "\n",
      "illness\n",
      "\n",
      ".099*\n",
      "\n",
      "0.077\n",
      "\n",
      "0.067\n",
      "\n",
      "outdoors\n",
      "\n",
      "-0.063\n",
      "\n",
      "-0.043\n",
      "\n",
      "-0.044\n",
      "\n",
      "biology\n",
      "\n",
      ".085*\n",
      "\n",
      "0.054\n",
      "\n",
      "0.063\n",
      "\n",
      "psychology\n",
      "\n",
      "0.067\n",
      "\n",
      "0.019\n",
      "\n",
      "0.058\n",
      "\n",
      "people/social\n",
      "\n",
      "-.127**\n",
      "\n",
      "-0.076\n",
      "\n",
      "-.083*\n",
      "\n",
      "education\n",
      "\n",
      "0.027\n",
      "\n",
      "0.056\n",
      "\n",
      "-0.002\n",
      "\n",
      "healthcare\n",
      "\n",
      "0.073\n",
      "\n",
      ".096*\n",
      "\n",
      "0.027\n",
      "\n",
      "5. epistemic network analysis\n",
      "ena measures the connections between elements in data and\n",
      "represents them in dynamic network models. ena creates these\n",
      "network models in a metric space that enables the comparison of\n",
      "networks in terms of (a) difference graph that highlights how the\n",
      "weighted connections of one network differ from another; and (b)\n",
      "statistics that summarize the weighted structure of network connections, enabling comparisons of many networks at once.\n",
      "ena was originally developed to model cognitive networks involved in complex thinking. these cognitive networks represent\n",
      "associations between knowledge, skills, habits of mind of individual learners or groups of learners. in this study, we used ena to\n",
      "construct network models. for each individual student, we constructed an ena network using the selected seven topic scores for\n",
      "each utterance the student contributed to the group.\n",
      "\n",
      "5.1 process\n",
      "while the process of creating ena models is described in more\n",
      "detail elsewhere (e.g. [11; 17-19]), we will briefly describe how\n",
      "ena models are created based on topic modeling. here we defined network nodes as the seven topics identified from the topic\n",
      "model. we defined the connections between nodes, or edges, as\n",
      "the strength of the co-occurrence of topics within a moving stanza\n",
      "window (msw) of size 5 [19]. to model connections between\n",
      "topics we used the products of the topic scores summed across all\n",
      "chats in the msw. that is, for each topic, the topic scores are\n",
      "summed across all 5 chats in the msw. then ena computed the\n",
      "product of the summed topic loadings for each pair topics to\n",
      "measure the strength of their co-occurrence. for example, if the\n",
      "sum of the topics scores across five chats was 0.5 for â€œillnessâ€, 0.3\n",
      "for â€œpsychologyâ€, and 0.2 for â€œhealthcareâ€, these scores would\n",
      "result in three co-occurrences, â€œillness-psychologyâ€, â€œillnesshealthcareâ€, and â€œpsychology-healthcareâ€, with scores of 0.15,\n",
      "0.1, and 0.06, respectively.\n",
      "next ena created adjacency matrices for each student that quantified the co-occurrences of topics within the studentsâ€™ discourse\n",
      "in the context of their chat group. subsequently, the adjacency\n",
      "matrices were then treated as vectors in a high dimensional space,\n",
      "where each dimension corresponds to co-occurrence of a pair of\n",
      "topics. the vectors were then normalized to unit vectors. notice\n",
      "that the normalization removed the effect of chat length embedded\n",
      "in the topic scores. a singular value decomposition (svd) was\n",
      "then performed for dimensional reduction. ena then projected a\n",
      "vector for each student into a low dimensional space that maximizes the variance explained in the data. finally, the nodes of the\n",
      "\n",
      "networks, which in this case correspond to the seven selected\n",
      "topics generated from tasa corpus, were placed in the low dimensional space. the topic nodes were placed using an optimization algorithm such that the overall distances between centroids\n",
      "(centers of the mass of the networks) and the corresponding projected student locations was minimized. a critical feature of ena\n",
      "is that these node placements are fixed, that is, the nodes of each\n",
      "network are in the same place for all units in the analysis. this\n",
      "fixing of the location of the nodes allows for meaningful comparisons between networks in terms of their connection patterns\n",
      "which allow us to interpret the metric space. as a result, ena\n",
      "produced two coordinated representations: (1) the location of each\n",
      "student in a projected metric space, in which all units of analysis\n",
      "included in the model were located, and (2) weighted network\n",
      "graphs for each student, which explained why the student was\n",
      "positioned where it was in the space.\n",
      "ena also allows us to compare the mean network graphs and\n",
      "mean position in ena space between different groups of students. in this study, we only considered the students with high\n",
      "potential to learn, i.e., the 624 students with pretest score < 0.5\n",
      "(50% correct). among these students, we compared the networks\n",
      "of low learning gain students (gain<-0.1, ğ‘=194) with the networks of high learning gain students (gain>0.43, ğ‘=105). we\n",
      "compared these groups using difference network graph, which\n",
      "was formed by subtracting the edge weights of the mean discourse\n",
      "network for the low gain group students from the mean discourse\n",
      "network from the high gain group. this difference network graph\n",
      "shows us which topic connections are stronger for each group. in\n",
      "addition, we conducted a t-test to test the difference between\n",
      "group means.\n",
      "\n",
      "5.2 results\n",
      "figure 3 shows mean discourse networks for students with low\n",
      "gain scores (left, red), students with high gain scores (right, blue),\n",
      "and a difference network graph (center) that shows how the discourse patterns of each group differs. students with low gains had\n",
      "stronger connections between the â€œpeople/socialâ€ topic and all\n",
      "other topics except for â€œillnessâ€. more importantly, the connection that was the strongest for low gain students compared to high\n",
      "gain students was between â€œpeople/socialâ€ and â€œoutdoorsâ€. students with high gain scores made stronger connections between\n",
      "the topics of â€œillnessâ€, â€œpsychologyâ€, â€œhealthcareâ€, â€œbiologyâ€, and\n",
      "â€œeducationâ€.\n",
      "table 4. comparison of centroids between low gain and high\n",
      "gain students, ğ’‘ = ğŸ. ğŸğŸ’ğŸ•, ğ’• = ğŸ. ğŸğŸ\n",
      "n\n",
      "\n",
      "mean\n",
      "\n",
      "sd\n",
      "\n",
      "high gain\n",
      "\n",
      "105\n",
      "\n",
      "0.033\n",
      "\n",
      "0.220\n",
      "\n",
      "low gain\n",
      "\n",
      "194\n",
      "\n",
      "-0.048\n",
      "\n",
      "0.322\n",
      "\n",
      "figure 4 shows centroids, or the centers of mass, of individual\n",
      "studentsâ€™ discourse networks and their means with low gain score\n",
      "students in red and high gain score students in blue. the differences between these two groups were significant on the x dimensions (see table 4). this means that the differences we saw in\n",
      "figure 2 and described above are statistically significant. in other\n",
      "words, the high learning gain studentsâ€™ discourse was more towards the right side of the ena space and the low learning gain\n",
      "studentsâ€™ discourse was more towards the left side. that indicates\n",
      "that the discourse of students with high learning gains made more\n",
      "connections between on-task topics (â€œillnessâ€, â€œpsychologyâ€,\n",
      "â€œhealthcareâ€, â€œbiologyâ€, and â€œeducationâ€), while the discourse of\n",
      "\n",
      "\n",
      "\n",
      "108\n",
      "\n",
      "\f",
      "low gain students made more connections between off-task topics\n",
      "(â€œpeople/socialâ€ and â€œoutdoorsâ€).\n",
      "\n",
      "6. discussion\n",
      "ena makes it possible to visualize the chat dynamics to help\n",
      "researchers gain deeper understanding of what is going on in a\n",
      "collaborative learning environment. differences in what topics\n",
      "students connect in discourse can predict learning outcomes. previous use of ena has relied on human coded data or use of regular expressions to classify data. utilizing topic modeling can lead\n",
      "to fully automated ena, making it more accessible to a wider\n",
      "group of researchers and allows ena to be used with more and\n",
      "larger data sets.\n",
      "the fact that the epistemic network predicts learning validates\n",
      "further application of ena. for example, the turn by turn chat\n",
      "dynamics can be plotted as trajectories in the 2-d space, where the\n",
      "\n",
      "topics are placed. investigating the trajectory patterns and their\n",
      "relationship to learning or socio-affective components are interesting future research directions.\n",
      "we used a general topic model in this study. many studies in the\n",
      "literature used lda for topic modeling on relatively small corpora. this causes two problems. 1) lda topic models built upon\n",
      "small corpora are not reliable, because lda requires large number documents with relatively large size for each document. inadequate corpus can result in misleading results. 2) using a topic\n",
      "model that is not common would result in arbitrary interpretation.\n",
      "for example, the representation of â€œillnessâ€ from different corpus\n",
      "could be very different. therefore, it is hard to compare the claims\n",
      "made to â€œillnessâ€ across different studies. using a reliable, common topic models will set up a common language for different\n",
      "studies.\n",
      "\n",
      "figure 3: mean discourse networks for students with low gain scores (left, red), students with high gain scores (right, blue), and a\n",
      "difference network graph (center).\n",
      "chat utterances are too short. the statistical inference algorithm\n",
      "contains a high degree of randomness for short documents. as an\n",
      "extreme example, an utterance with a single word, would result in\n",
      "inferred topic proportion scores with â€œ1â€ on one topic and â€œ0â€ on\n",
      "others. the problem is that, this â€œ1â€ was assigned to a topic with\n",
      "certain degree of uncertainty. that is, the topic this â€œ1â€ was assigned to could be any topic. while aggregated analysis may not\n",
      "be sensitive to such uncertainty, detailed utterance by utterance\n",
      "analysis would suffer from it.\n",
      "our method of computing topic scores is based on the topic probability distribution over each word. we treat the topic distribution\n",
      "of each word as a vector. when computing the topic score, the\n",
      "simple sum of all word vectors gives scores to all topics. as we\n",
      "have pointed out, the summation algorithm will have a length\n",
      "effect. therefore, when such topic scores are used, removing\n",
      "length effects through normalization is necessary. in this article,\n",
      "we did not use weighted sum as suggested in cai et al. [4]. comparing the effect of different weighting is beyond the scope of this\n",
      "paper.\n",
      "figure 4: discourse network centroids low gain score students\n",
      "red, high gain score students blue.\n",
      "topic scores for documents are usually inferred from topic models. while for longer documents, the topic scores can be used in\n",
      "many applications (e.g., text clustering [1]), the inferred topic\n",
      "proportion scores wonâ€™t be useful for analyzing chats if we need\n",
      "to treat each utterance as a unit of analysis. it is not useful because\n",
      "\n",
      "when a general topic model is used, selecting topics relevant to\n",
      "the specific analysis becomes important. our approach was to\n",
      "look at the total scores of utterances and find the â€œhotâ€ topics by\n",
      "sorting the total topic scores. in our study, we had a quickly decreasing curve that helped us to select topics. we believe this\n",
      "would be the case for most studies using a model containing far\n",
      "more topics than the topics contained in the target data.\n",
      "\n",
      "\n",
      "\n",
      "109\n",
      "\n",
      "\f",
      "although our study started with topic modeling to capture the\n",
      "â€œwhatâ€ in the chats, the association networks constructed in the\n",
      "epistemic network analysis actually turned the â€œwhatâ€ into a\n",
      "â€œhowâ€: how the topics in the chats associated with each other.\n",
      "this is conceptually similar to the cohesion features dowell [7]\n",
      "and cade [3] used.\n",
      "topic modeling emphasizes content words. when a topic model is\n",
      "built, stop words are usually removed. an interesting question is,\n",
      "what if we do the opposite: keep stop words and remove content\n",
      "words? pennebaker (e.g., [13]) laid foundational work in this direction. the liwc tool pennebaker and his colleagues created\n",
      "provides over a hundred text measures by counting non-content\n",
      "words. liwc measures could provide different features to epistemic network analysis and reveal different aspects of the chat\n",
      "dynamics.\n",
      "\n",
      "7. acknowledgments\n",
      "the research on was supported by the national science foundation (drk-12-0918409, drk-12 1418288), the institute of education sciences (r305c120001), army research lab (w911inf12-2-0030), and the office of naval research (n00014-12-c0643; n00014-16-c-3027). any opinions, findings, and conclusions or recommendations expressed in this material are those of\n",
      "the authors and do not necessarily reflect the views of nsf, ies,\n",
      "or dod. the tutoring research group (trg) is an interdisciplinary research team comprised of researchers from psychology,\n",
      "computer science, and other departments at university of memphis (visit http://www.autotutor.org).\n",
      "\n",
      "8. references\n",
      "[1]\n",
      "\n",
      "alghamdi, r. and alfalqi, k. 2015. a survey of topic\n",
      "modeling in text mining. ijacsa) international journal\n",
      "of advanced computer science and applications. 6, 1\n",
      "(2015), 147â€“153.\n",
      "\n",
      "[2]\n",
      "\n",
      "blei, d.m., edu, b.b., ng, a.y., edu, a.s., jordan, m.i.\n",
      "and edu, j.b. 2003. latent dirichlet allocation. journal\n",
      "of machine learning research. 3, (2003), 993â€“1022.\n",
      "\n",
      "[3]\n",
      "\n",
      "cade, w.l., dowell, n.m.m. and pennebaker, j. 2014.\n",
      "modeling student socioaffective responses to group\n",
      "interactions in a collaborative online chat environment.\n",
      "proceedings of the 7th international conference on\n",
      "educational data mining (edm). 2, 21 (2014), 399â€“400.\n",
      "\n",
      "[4]\n",
      "\n",
      "[5]\n",
      "\n",
      "cai, z., li, h., graesser, a.c. and hu, x. 2016. can\n",
      "word probabilities from lda be simply added up to\n",
      "represent documentsâ€¯? proceedings of the 9th\n",
      "international conference on educational data mining.\n",
      "(2016), 577â€“578.\n",
      "von davier, a.a. and halpin, p.f. 2013. collaborative\n",
      "problem-solving and the assessment of cognitive skills:\n",
      "psychometric considerations. ets research report\n",
      "series. december (2013), 36 p.\n",
      "\n",
      "[6]\n",
      "\n",
      "dillenbourg, p. and traum, d. 2006. sharing solutions:\n",
      "persistence and grounding in multimodal collaborative\n",
      "problem solving. the journal of the learning sciences.\n",
      "15, 1 (2006), 121â€“151.\n",
      "\n",
      "[7]\n",
      "\n",
      "dowell, n., cade, w.â€¯, tausczik, y., pennebaker, j., and\n",
      "graesser, a. 2014. what works: creating adaptive and\n",
      "intelligent systems for collaborative learning support.\n",
      "springer international publishing switzerland. (2014),\n",
      "124â€“133.\n",
      "\n",
      "[8]\n",
      "\n",
      "dowell, n.m.m., skrypnyk, s., joksimoviÄ‡, s., graesser,\n",
      "\n",
      "a., dawson, s., gaÅ¡eviÄ‡, d., hennis, t. a., vries, p. de\n",
      "and kovanoviÄ‡, v. 2015. modeling learners â€™ social\n",
      "centrality and performance through language and\n",
      "discourse. educational data mining - edmâ€™15 (2015),\n",
      "250â€“257.\n",
      "[9]\n",
      "\n",
      "fiore, s.m., rosen, m. a., smith-jentsch, k. a., salas, e.,\n",
      "letsky, m. and warner, n. 2010. toward an\n",
      "understanding of macrocognition in teams: predicting\n",
      "processes in complex collaborative contexts. human\n",
      "factors. 52, 2 (2010), 203â€“224.\n",
      "\n",
      "[10]\n",
      "\n",
      "graesser, a.c., mcnamara, d.s., louwerse, m.m. and\n",
      "cai, z. 2004. coh-metrix: analysis of text on cohesion\n",
      "and language. behavior research methods, instruments,\n",
      "& computers. 36, 2 (2004), 193â€“202.\n",
      "\n",
      "[11]\n",
      "\n",
      "li, h., samei, b., olney, a., graesser, a. and shaffer, d.\n",
      "2014. question classification in an epistemic game.\n",
      "international conference on intelligent tutoring\n",
      "systems. (2014).\n",
      "\n",
      "[12]\n",
      "\n",
      "pennebaker, j.w., boyd, r.l., jordan, k. and blackburn,\n",
      "k. 2015. the development and psychometric properties\n",
      "of liwc2015. austin, tx: university of texas at austin.\n",
      "(2015).\n",
      "\n",
      "[13]\n",
      "\n",
      "pennebaker, j.w., chung, c.k., frazee, j. and lavergne,\n",
      "g.m. 2014. when small words foretell academic\n",
      "successâ€¯: the case of college admissions essays.\n",
      "(2014), 1â€“10.\n",
      "\n",
      "[14]\n",
      "\n",
      "rosen, y. 2014. assessing collaborative problem\n",
      "solving through computer agent technologies.\n",
      "encyclopedia of information science and technology. 9,\n",
      "november (2014), 94â€“102.\n",
      "\n",
      "[15]\n",
      "\n",
      "sawyer, r.k. 2014. the new science of learning. the\n",
      "cambridge handbook of the learning sciences. 1â€“18.\n",
      "\n",
      "[16]\n",
      "\n",
      "scholand, a.j., tausczik, y.r. and pennebaker, j.w.\n",
      "2010. assessing group interaction with social language\n",
      "network analysis. lecture notes in computer science\n",
      "(including subseries lecture notes in artificial\n",
      "intelligence and lecture notes in bioinformatics). 6007\n",
      "lncs, (2010), 248â€“255.\n",
      "\n",
      "[17]\n",
      "\n",
      "shaffer, d.w. 2006. epistemic frames for epistemic\n",
      "games. computers and education. 46, 3 (2006), 223â€“\n",
      "234.\n",
      "\n",
      "[18]\n",
      "\n",
      "shaffer, d.w., hatfield, d., svarovsky, g.n., nash, p.,\n",
      "nulty, a., bagley, e., frank, k., rupp, a.a. and\n",
      "mislevy, r.j. 2009. epistemic network analysis: a\n",
      "prototype for 21st-century assessment of learning.\n",
      "international journal of learning and media. 1, 2\n",
      "(2009), 33â€“53.\n",
      "\n",
      "[19]\n",
      "\n",
      "siebert-evenstone, a.l., arastoopour, g., collier, w.,\n",
      "swiecki, z., ruis, a.r. and shaffer, d.w. 2016. in\n",
      "search of conversational grain size: modeling semantic\n",
      "structure using moving stanza windows. international\n",
      "conference of the learning sciences. (2016).\n",
      "\n",
      "[20]\n",
      "\n",
      "slavin, r.e. 1995. cooperative learning: theory,\n",
      "research and practice (2nd ed.). the nature of learning.\n",
      "(1995), 208.\n",
      "\n",
      "[21]\n",
      "\n",
      "tuulos, v.h. and tirri, h. 2004. combining topic\n",
      "models and social networks for chat data mining.\n",
      "proceedings of the 2004 ieee/wic/acm international\n",
      "conference on web intelligence. october (2004), 206â€“\n",
      "\n",
      "\n",
      "\n",
      "110\n",
      "\n",
      "\f",
      "213.\n",
      "[22]\n",
      "[23]\n",
      "\n",
      "whitepaper, a.r. 2014. what happens when we learn\n",
      "together. (2014).\n",
      "yousef, a.m.f., chatti, m.a., schroeder, u., wosnitza,\n",
      "m. and jakobs, h. 2014. a review of the state-of-theart. proceedings of the 6th international conference on\n",
      "\n",
      "computer supported education - csedu2014. (2014),\n",
      "9â€“20.\n",
      "[24]\n",
      "\n",
      "wang z., qiu b., bai, w., chuan, s. and le, y. 2014.\n",
      "collapsed gibbs sampling for latent dirichlet\n",
      "allocation on spark. jmlr: workshop and conference\n",
      "proceedings. 2004 (2014), 17â€“28.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12) iterate through each of the text files and read their contents in the variable below:\n",
    "# Using a for loop, iterate over all the files in the directory, and add them to the list below\n",
    "text_list = []\n",
    "\n",
    "for paper in os.listdir('Papers'):\n",
    "    file = open('Papers/'+ paper,'r') \n",
    "    contents = file.read()\n",
    "    text_list.append(contents)\n",
    "    \n",
    "print(text_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) Now we are going to compute the frequency of each word across all \n",
    "# documents. Feel free to use the link below to help you!\n",
    "# hint: https://www.datacamp.com/community/tutorials/absolute-weighted-word-frequency\n",
    "# (look at the first block of code in the article)\n",
    "# Using the text_list we create in the cell above, iterate over all words and count their frequencies\n",
    "# If uncomfortable with dictionaries, google python dict\n",
    "word_freq = defaultdict(int)\n",
    "\n",
    "for text in text_list:\n",
    "    for word in text.split():\n",
    "        word_freq[word] += 1\n",
    "\n",
    "df = pd.DataFrame.from_dict(word_freq, orient='index') \\\n",
    ".sort_values(0, ascending=False) \\\n",
    ".rename(columns={0: 'abs_freq'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abs_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>5663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>3402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>2704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>2406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>2028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     abs_freq\n",
       "the      5663\n",
       "of       3402\n",
       "and      2704\n",
       "to       2406\n",
       "a        2028"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 14) If you haven't done so already, create a dataframe from the dictionary\n",
    "# and print the head of the dataframe\n",
    "# Just as we did last week with Pandas, we can do this in only a few lines\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's a problem with the dataframe above? Is there data meaningful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abs_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>students</th>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student</th>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>=</th>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          abs_freq\n",
       "learning       638\n",
       "data           512\n",
       "students       421\n",
       "student        407\n",
       "=              393"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 15) We are going to remove the following stop words, so that we see more interesting \n",
    "# keywors. Feel free to use the list and hint below to help you:\n",
    "# hint: https://stackoverflow.com/questions/43716402/remove-row-index-dataframe-pandas\n",
    "# the .drop() function could prove useful here\n",
    "STOPWORDS = ['a','able','about','across','after','all','almost','also','am','among',\n",
    "             'an','and','any','are','as','at','be','because','been','but','by','can',\n",
    "             'cannot','could','dear','did','do','does','either','else','ever','every',\n",
    "             'for','from','get','got','had','has','have','he','her','hers','him','his',\n",
    "             'how','however','i','if','in','into','is','it','its','just','least','let',\n",
    "             'like','likely','may','me','might','most','must','my','neither','no','nor',\n",
    "           'not','of','off','often','on','only','or','other','our','own','rather','said',\n",
    "             'say','says','she','should','since','so','some','than','that','the','their',\n",
    "             'them','then','there','these','they','this','tis','to','too','twas','us',\n",
    "             'wants','was','we','were','what','when','where','which','while','who',\n",
    "             'whom','why','will','with','would','yet','you','your']\n",
    "\n",
    "# for word in STOPWORDS:\n",
    "#     df.drop([word], axis=0, inplace=True)\n",
    "\n",
    "df.drop(STOPWORDS, axis=0, inplace=True, errors='ignore')\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             abs_freq\n",
      "learning          638\n",
      "data              512\n",
      "students          421\n",
      "student           407\n",
      "=                 393\n",
      "each              350\n",
      "model             345\n",
      "more              310\n",
      "using             278\n",
      "used              258\n",
      "performance       241\n",
      "between           231\n",
      "number            213\n",
      "two               207\n",
      "based             198\n",
      "set               184\n",
      "different         179\n",
      "educational       173\n",
      "models            172\n",
      "results           171\n"
     ]
    }
   ],
   "source": [
    "# 16) print the top 20 words of your new dataframe: we can do this with a list slice \n",
    "top20 = df[:20]\n",
    "print(top20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x119f4e400>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 17 plot the top 20 results above as a histogram: \n",
    "top20.plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you tell from this histogram? What do EDM researchers seem to care about?\n",
    "\n",
    "EDM researchers really care about learning! Students! Performance of models!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are improvements you could add to our data cleaning process? Write at least three things:\n",
    "* cut out the equal sign and other numbers (from rows)\n",
    "* combine rows with plurals of the words (student v. students)\n",
    "* remove written number words (like \"two\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count word frequencies per paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the previous section gave us an overall description of the word frequency for all the papers, it would be interesting to look at each individual paper. This is what we are going to do below, by focusing on the top 30 terms used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) save the top 30 words from the dataframe above \n",
    "# in a new variable called \"top_words\"\n",
    "\n",
    "top_words = df[:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'Dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-95dfdd98cd7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# dfCurrent.loc[[0],['text']] = paperz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdfNew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mdfNew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'Dataframe'"
     ]
    }
   ],
   "source": [
    "# 19) We are now going to construct a new dataframe where each row is a paper, \n",
    "# each column is one of the top 30 words used and each cell is a count of this word. \n",
    "# NOTE: make sure you add another field called \"text\" where you're going to store the \n",
    "# actual text of the paper. \n",
    "# Hint: build a list of dataframes (one for each papers), \n",
    "# and use the concat function from pandas to concatenate them!\n",
    "# paper  word1   word2   word3\n",
    "d = []\n",
    "\n",
    "\n",
    "#text in text_list is each paper\n",
    "for text in text_list:\n",
    "    dic = {}\n",
    "    dic['text'] = text\n",
    "    for word in text.split():\n",
    "        for word in top_words:\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for word in top_words:\n",
    "#         for word in text.split():\n",
    "        \n",
    "        \n",
    "        \n",
    "# for word in text.split():\n",
    "#         word_freq[word] += 1\n",
    "\n",
    "# df = pd.DataFrame.from_dict(word_freq, orient='index') \\\n",
    "# .sort_values(0, ascending=False) \\\n",
    "# .rename(columns={0: 'abs_freq'})\n",
    "\n",
    "        \n",
    "#     iterate through the top words, add counts to the dictionary\n",
    "#     and append the results to the list above (d)\n",
    "\n",
    "# concatenate the list d into a dataframe\n",
    "\n",
    "pd.DataFrame(dict)\n",
    "\n",
    "# for paper in papers:\n",
    "\n",
    "# paperz = papers[0]\n",
    "# print(paperz)\n",
    "\n",
    "# dfCurrent.iloc[[0],['text']] = paperz\n",
    "\n",
    "# dfCurrent = pd.DataFrame(columns=[text_list])\n",
    "# dfCurrent.loc[[0],['text']] = paperz\n",
    "\n",
    "dfNew = pd.Dataframe()\n",
    "dfNew.columns = top_words\n",
    "\n",
    "dfNew.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20) create a scatter plot of the words 'learning' and 'data'\n",
    "# what can you say from it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21) annotate each point with the index number of the dataframe\n",
    "# hint: https://www.pythonmembers.club/2018/05/08/matplotlib-scatter-plot-annotate-set-text-at-label-each-point/\n",
    "# plt.txt( ) is going to be helpful for us here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22) what are the two extreme papers, \n",
    "# i.e., papers with more occurences for each term on each axis?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23) plot the histogram of the paper that had high counts of \"data\"\n",
    "# hint: https://stackoverflow.com/questions/52392728/create-a-histogram-based-on-one-row-of-a-dataframe\n",
    "# .loc is going to be helpful here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24) plot the histogram of the paper that had high counts of \"learning\"\n",
    "# .loc is going to be helpful here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25) what can you observe? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26) print the first 1000 characters of each paper. \n",
    "\n",
    "\n",
    "# Is your interpretation confirmed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to work with Regex formulas to extract part of the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27) we are going to work on the first paper to make sure that our \n",
    "# regex works. Just retrieve the text and assign it to a variable below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28) find the text between the words 'abstract' and \n",
    "# 'introduction' for the first paper using a regex\n",
    "# https://stackoverflow.com/questions/12736074/regex-matching-between-two-strings/12736203\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 29) find the text between the words 'abstract' and \n",
    "# 'introduction' for the first paper using the .index() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30) add a new column namd \"abstract\" to the dataframe above \n",
    "# and initialize it with an empty string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now add the abstracts to each row of the dataframe using either\n",
    "# of the two methods above\n",
    "# Hint: https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31) print your abstracts (they should contain a lot of \\n = carriage return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32) clean the abstract column using the \"apply\" function with a lambda\n",
    "# Hint: https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.Series.apply.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing documents using TF-IDF (this to below is optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33) now we are going to do something a little more advanced:'\n",
    "# we are going to compute the similarity between two texts\n",
    "# using a method called tf-idf (we'll talk more about it later)\n",
    "# Hint: https://stackoverflow.com/questions/43631533/similarity-between-two-text-documents-in-python\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34) What can you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 35) repeat the same procedure with the entire papers\n",
    "# Use the same logic as the previous cell, but use the text_list variable that we defined previously\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 36) What are two documents that seem to be very similar?\n",
    "# print their abstract: \n",
    "# print the first 1000 characters of each paper. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 37) what seems to be similar between them? \n",
    "\n",
    "# they both talk about analyzing questions and answers from students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free exploration (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- try to extract the names of the author\n",
    "- find a way to get the top words shared across two texts\n",
    "- use a regex (or any other method) to get the list of references"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
